import wikipedia
import deepl
import csv
# importing BART
from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig
# Create a Translator object providing your DeepL API authentication key
translator = deepl.Translator(“c1df6af4-2c84-881c-5528-c367bf21e5f0:fx”)
# Loading the model and tokenizer for bart-large-cnn
tokenizer=BartTokenizer.from_pretrained(‘facebook/bart-large-cnn’)
model=BartForConditionalGeneration.from_pretrained(‘facebook/bart-large-cnn’)
with open(‘Monuments.csv’) as csv_file:
  csv_reader = csv.reader(csv_file, delimiter=‘,’)
  line_count = 0
  for row in csv_reader:
      print({row[2]})
      line_count += 1
      wikipedia.set_lang(“FR”)
      wikisearch = wikipedia.page({row[2]})
      wikioriginal = wikisearch.content
# Translate text into a target language, in this case,English
      # wikicontent = translator.translate_text(wikioriginal, target_lang=“EN-GB”)
      wikicontent = wikioriginal
      print(type(wikicontent))
      wikiclean = str(wikicontent)
      inputs = tokenizer.batch_encode_plus([wikiclean],return_tensors=‘pt’,truncation=True)
      summary_ids = model.generate(inputs[‘input_ids’], early_stopping=True)
# Decoding and printing the summary
      final_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
      print(final_summary)
#print(wikifinal)
#Printing the summarized description in new CSV
      with open(‘MonumentsEdited.csv’, mode=‘a’) as final_file :
        employee_writer = csv.writer(final_file, delimiter=‘,’, quotechar=‘“’, quoting=csv.QUOTE_MINIMAL)
        employee_writer.writerow([final_summary])
